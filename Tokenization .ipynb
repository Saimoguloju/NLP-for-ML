{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2f0902b",
   "metadata": {},
   "source": [
    "![Python logo](https://raw.githubusercontent.com/satishgunjal/images/master/NLP_Header_Tokenization.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e35392bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21e10809",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\"I am Moguloju Sai, a Data Science Engineer actively seeking new opportunities. I am building a Telugu Data Science community and sharing content through my YouTube and WhatsApp channels. If you're interested in joining the community, please visit my LinkedIn page for more information!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "520dc56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am Moguloju Sai, a Data Science Engineer actively seeking new opportunities. I am building a Telugu Data Science community and sharing content through my YouTube and WhatsApp channels. If you're interested in joining the community, please visit my LinkedIn page for more information!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb79ab00",
   "metadata": {},
   "source": [
    "This line imports the sent_tokenize function from the nltk.tokenize module. This function is specifically designed to break down a given text into individual sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8f900db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I am Moguloju Sai, a Data Science Engineer actively seeking new opportunities.', 'I am building a Telugu Data Science community and sharing content through my YouTube and WhatsApp channels.', \"If you're interested in joining the community, please visit my LinkedIn page for more information!\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sentences = sent_tokenize(corpus)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2df6caa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24d764bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am Moguloju Sai, a Data Science Engineer actively seeking new opportunities.\n",
      "I am building a Telugu Data Science community and sharing content through my YouTube and WhatsApp channels.\n",
      "If you're interested in joining the community, please visit my LinkedIn page for more information!\n"
     ]
    }
   ],
   "source": [
    "for lines in sentences:\n",
    "    print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16e47db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method - 1\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62ea8d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a2a6e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'am',\n",
       " 'Moguloju',\n",
       " 'Sai',\n",
       " ',',\n",
       " 'a',\n",
       " 'Data',\n",
       " 'Science',\n",
       " 'Engineer',\n",
       " 'actively',\n",
       " 'seeking',\n",
       " 'new',\n",
       " 'opportunities',\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'building',\n",
       " 'a',\n",
       " 'Telugu',\n",
       " 'Data',\n",
       " 'Science',\n",
       " 'community',\n",
       " 'and',\n",
       " 'sharing',\n",
       " 'content',\n",
       " 'through',\n",
       " 'my',\n",
       " 'YouTube',\n",
       " 'and',\n",
       " 'WhatsApp',\n",
       " 'channels',\n",
       " '.',\n",
       " 'If',\n",
       " 'you',\n",
       " \"'re\",\n",
       " 'interested',\n",
       " 'in',\n",
       " 'joining',\n",
       " 'the',\n",
       " 'community',\n",
       " ',',\n",
       " 'please',\n",
       " 'visit',\n",
       " 'my',\n",
       " 'LinkedIn',\n",
       " 'page',\n",
       " 'for',\n",
       " 'more',\n",
       " 'information',\n",
       " '!']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d5b526a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'Moguloju', 'Sai', ',', 'a', 'Data', 'Science', 'Engineer', 'actively', 'seeking', 'new', 'opportunities', '.', 'I', 'am', 'building', 'a', 'Telugu', 'Data', 'Science', 'community', 'and', 'sharing', 'content', 'through', 'my', 'YouTube', 'and', 'WhatsApp', 'channels', '.', 'If', 'you', \"'re\", 'interested', 'in', 'joining', 'the', 'community', ',', 'please', 'visit', 'my', 'LinkedIn', 'page', 'for', 'more', 'information', '!']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "287ddbd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28b000d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'Moguloju', 'Sai', ',', 'a', 'Data', 'Science', 'Engineer', 'actively', 'seeking', 'new', 'opportunities', '.']\n",
      "['I', 'am', 'building', 'a', 'Telugu', 'Data', 'Science', 'community', 'and', 'sharing', 'content', 'through', 'my', 'YouTube', 'and', 'WhatsApp', 'channels', '.']\n",
      "['If', 'you', \"'re\", 'interested', 'in', 'joining', 'the', 'community', ',', 'please', 'visit', 'my', 'LinkedIn', 'page', 'for', 'more', 'information', '!']\n"
     ]
    }
   ],
   "source": [
    "for lines in sentences:\n",
    "    print(word_tokenize(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "512d83c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method -2\n",
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6915d5fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'am',\n",
       " 'Moguloju',\n",
       " 'Sai',\n",
       " ',',\n",
       " 'a',\n",
       " 'Data',\n",
       " 'Science',\n",
       " 'Engineer',\n",
       " 'actively',\n",
       " 'seeking',\n",
       " 'new',\n",
       " 'opportunities',\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'building',\n",
       " 'a',\n",
       " 'Telugu',\n",
       " 'Data',\n",
       " 'Science',\n",
       " 'community',\n",
       " 'and',\n",
       " 'sharing',\n",
       " 'content',\n",
       " 'through',\n",
       " 'my',\n",
       " 'YouTube',\n",
       " 'and',\n",
       " 'WhatsApp',\n",
       " 'channels',\n",
       " '.',\n",
       " 'If',\n",
       " 'you',\n",
       " \"'\",\n",
       " 're',\n",
       " 'interested',\n",
       " 'in',\n",
       " 'joining',\n",
       " 'the',\n",
       " 'community',\n",
       " ',',\n",
       " 'please',\n",
       " 'visit',\n",
       " 'my',\n",
       " 'LinkedIn',\n",
       " 'page',\n",
       " 'for',\n",
       " 'more',\n",
       " 'information',\n",
       " '!']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d4154bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_punct = wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36912c3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8418a289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method - 3\n",
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62967c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9cb5626e",
   "metadata": {},
   "outputs": [],
   "source": [
    "treebank = tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e49a510a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'am',\n",
       " 'Moguloju',\n",
       " 'Sai',\n",
       " ',',\n",
       " 'a',\n",
       " 'Data',\n",
       " 'Science',\n",
       " 'Engineer',\n",
       " 'actively',\n",
       " 'seeking',\n",
       " 'new',\n",
       " 'opportunities.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'building',\n",
       " 'a',\n",
       " 'Telugu',\n",
       " 'Data',\n",
       " 'Science',\n",
       " 'community',\n",
       " 'and',\n",
       " 'sharing',\n",
       " 'content',\n",
       " 'through',\n",
       " 'my',\n",
       " 'YouTube',\n",
       " 'and',\n",
       " 'WhatsApp',\n",
       " 'channels.',\n",
       " 'If',\n",
       " 'you',\n",
       " \"'re\",\n",
       " 'interested',\n",
       " 'in',\n",
       " 'joining',\n",
       " 'the',\n",
       " 'community',\n",
       " ',',\n",
       " 'please',\n",
       " 'visit',\n",
       " 'my',\n",
       " 'LinkedIn',\n",
       " 'page',\n",
       " 'for',\n",
       " 'more',\n",
       " 'information',\n",
       " '!']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da6e2610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'Moguloju', 'Sai', ',', 'a', 'Data', 'Science', 'Engineer', 'actively', 'seeking', 'new', 'opportunities.', 'I', 'am', 'building', 'a', 'Telugu', 'Data', 'Science', 'community', 'and', 'sharing', 'content', 'through', 'my', 'YouTube', 'and', 'WhatsApp', 'channels.', 'If', 'you', \"'re\", 'interested', 'in', 'joining', 'the', 'community', ',', 'please', 'visit', 'my', 'LinkedIn', 'page', 'for', 'more', 'information', '!']\n"
     ]
    }
   ],
   "source": [
    "print(treebank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6e42411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(treebank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e40d73f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'am',\n",
       " 'Moguloju',\n",
       " 'Sai',\n",
       " ',',\n",
       " 'a',\n",
       " 'Data',\n",
       " 'Science',\n",
       " 'Engineer',\n",
       " 'actively',\n",
       " 'seeking',\n",
       " 'new',\n",
       " 'opportunities',\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'building',\n",
       " 'a',\n",
       " 'Telugu',\n",
       " 'Data',\n",
       " 'Science',\n",
       " 'community',\n",
       " 'and',\n",
       " 'sharing',\n",
       " 'content',\n",
       " 'through',\n",
       " 'my',\n",
       " 'YouTube',\n",
       " 'and',\n",
       " 'WhatsApp',\n",
       " 'channels',\n",
       " '.',\n",
       " 'If',\n",
       " \"you're\",\n",
       " 'interested',\n",
       " 'in',\n",
       " 'joining',\n",
       " 'the',\n",
       " 'community',\n",
       " ',',\n",
       " 'please',\n",
       " 'visit',\n",
       " 'my',\n",
       " 'LinkedIn',\n",
       " 'page',\n",
       " 'for',\n",
       " 'more',\n",
       " 'information',\n",
       " '!']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Method - 4\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "tweet = tweet_tokenizer.tokenize(corpus)\n",
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "23870cde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bdc32547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'am',\n",
       " 'Moguloju',\n",
       " 'Sai',\n",
       " 'a',\n",
       " 'Data',\n",
       " 'Science',\n",
       " 'Engineer',\n",
       " 'actively',\n",
       " 'seeking',\n",
       " 'new',\n",
       " 'opportunities',\n",
       " 'I',\n",
       " 'am',\n",
       " 'building',\n",
       " 'a',\n",
       " 'Telugu',\n",
       " 'Data',\n",
       " 'Science',\n",
       " 'community',\n",
       " 'and',\n",
       " 'sharing',\n",
       " 'content',\n",
       " 'through',\n",
       " 'my',\n",
       " 'YouTube',\n",
       " 'and',\n",
       " 'WhatsApp',\n",
       " 'channels',\n",
       " 'If',\n",
       " 'you',\n",
       " 're',\n",
       " 'interested',\n",
       " 'in',\n",
       " 'joining',\n",
       " 'the',\n",
       " 'community',\n",
       " 'please',\n",
       " 'visit',\n",
       " 'my',\n",
       " 'LinkedIn',\n",
       " 'page',\n",
       " 'for',\n",
       " 'more',\n",
       " 'information']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Method -5\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "regexp_tokenizer = RegexpTokenizer(r'\\w+')\n",
    "regex_tokens = regexp_tokenizer.tokenize(corpus)\n",
    "regex_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5d9e2a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(regex_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b1e626",
   "metadata": {},
   "source": [
    "###### NLTK offers a variety of tokenization techniques, each with its own strengths and weaknesses. Here's a breakdown of the key differences:\n",
    "\n",
    "1. Word Tokenization:\n",
    "\n",
    " \n",
    "- General-purpose tokenizer that splits text into words.\n",
    "- Considers punctuation and contractions.\n",
    "- Suitable for most standard text.\n",
    "\n",
    "2. Word Punctuation Tokenization:\n",
    "\n",
    " \n",
    "- Similar to word_tokenize, but treats punctuation as separate tokens.\n",
    "- Useful for tasks like sentiment analysis or information extraction, where punctuation can carry significant meaning.\n",
    "\n",
    "3. Treebank Word Tokenization:\n",
    "\n",
    " \n",
    "- Trained on the Penn Treebank corpus, a large annotated corpus of English text.\n",
    "- Handles contractions and other linguistic nuances more accurately.\n",
    "- Suitable for tasks requiring precise tokenization, such as parsing or named entity recognition.\n",
    "\n",
    "4. Tweet Tokenization:\n",
    "\n",
    " \n",
    "- Specifically designed for tokenizing text from social media platforms like Twitter.\n",
    "- Handles hashtags, mentions, emoticons, and other non-standard text features.\n",
    "- Useful for sentiment analysis or topic modeling on social media data.\n",
    "\n",
    "5. Regular Expression Tokenization:\n",
    "\n",
    "- Allows you to define custom tokenization patterns using regular expressions.\n",
    "- Provides flexibility for specific tokenization needs, such as handling domain-specific terms or non-standard text.\n",
    "- Requires more knowledge of regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96d365ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## To be continued"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f18503",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
